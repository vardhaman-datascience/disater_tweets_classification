{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import os\n",
    "import nltk \n",
    "import nltk.corpus\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from string import punctuation\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test=pd.read_csv(\"test.csv\")\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train=pd.read_csv(\"train.csv\")\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       61\n",
       "location    2533\n",
       "text           0\n",
       "target         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'ablaze', 'accident', 'aftershock', 'airplane%20accident',\n",
       "       'ambulance', 'annihilated', 'annihilation', 'apocalypse',\n",
       "       'armageddon', 'army', 'arson', 'arsonist', 'attack', 'attacked',\n",
       "       'avalanche', 'battle', 'bioterror', 'bioterrorism', 'blaze',\n",
       "       'blazing', 'bleeding', 'blew%20up', 'blight', 'blizzard', 'blood',\n",
       "       'bloody', 'blown%20up', 'body%20bag', 'body%20bagging',\n",
       "       'body%20bags', 'bomb', 'bombed', 'bombing', 'bridge%20collapse',\n",
       "       'buildings%20burning', 'buildings%20on%20fire', 'burned',\n",
       "       'burning', 'burning%20buildings', 'bush%20fires', 'casualties',\n",
       "       'casualty', 'catastrophe', 'catastrophic', 'chemical%20emergency',\n",
       "       'cliff%20fall', 'collapse', 'collapsed', 'collide', 'collided',\n",
       "       'collision', 'crash', 'crashed', 'crush', 'crushed', 'curfew',\n",
       "       'cyclone', 'damage', 'danger', 'dead', 'death', 'deaths', 'debris',\n",
       "       'deluge', 'deluged', 'demolish', 'demolished', 'demolition',\n",
       "       'derail', 'derailed', 'derailment', 'desolate', 'desolation',\n",
       "       'destroy', 'destroyed', 'destruction', 'detonate', 'detonation',\n",
       "       'devastated', 'devastation', 'disaster', 'displaced', 'drought',\n",
       "       'drown', 'drowned', 'drowning', 'dust%20storm', 'earthquake',\n",
       "       'electrocute', 'electrocuted', 'emergency', 'emergency%20plan',\n",
       "       'emergency%20services', 'engulfed', 'epicentre', 'evacuate',\n",
       "       'evacuated', 'evacuation', 'explode', 'exploded', 'explosion',\n",
       "       'eyewitness', 'famine', 'fatal', 'fatalities', 'fatality', 'fear',\n",
       "       'fire', 'fire%20truck', 'first%20responders', 'flames',\n",
       "       'flattened', 'flood', 'flooding', 'floods', 'forest%20fire',\n",
       "       'forest%20fires', 'hail', 'hailstorm', 'harm', 'hazard',\n",
       "       'hazardous', 'heat%20wave', 'hellfire', 'hijack', 'hijacker',\n",
       "       'hijacking', 'hostage', 'hostages', 'hurricane', 'injured',\n",
       "       'injuries', 'injury', 'inundated', 'inundation', 'landslide',\n",
       "       'lava', 'lightning', 'loud%20bang', 'mass%20murder',\n",
       "       'mass%20murderer', 'massacre', 'mayhem', 'meltdown', 'military',\n",
       "       'mudslide', 'natural%20disaster', 'nuclear%20disaster',\n",
       "       'nuclear%20reactor', 'obliterate', 'obliterated', 'obliteration',\n",
       "       'oil%20spill', 'outbreak', 'pandemonium', 'panic', 'panicking',\n",
       "       'police', 'quarantine', 'quarantined', 'radiation%20emergency',\n",
       "       'rainstorm', 'razed', 'refugees', 'rescue', 'rescued', 'rescuers',\n",
       "       'riot', 'rioting', 'rubble', 'ruin', 'sandstorm', 'screamed',\n",
       "       'screaming', 'screams', 'seismic', 'sinkhole', 'sinking', 'siren',\n",
       "       'sirens', 'smoke', 'snowstorm', 'storm', 'stretcher',\n",
       "       'structural%20failure', 'suicide%20bomb', 'suicide%20bomber',\n",
       "       'suicide%20bombing', 'sunk', 'survive', 'survived', 'survivors',\n",
       "       'terrorism', 'terrorist', 'threat', 'thunder', 'thunderstorm',\n",
       "       'tornado', 'tragedy', 'trapped', 'trauma', 'traumatised',\n",
       "       'trouble', 'tsunami', 'twister', 'typhoon', 'upheaval',\n",
       "       'violent%20storm', 'volcano', 'war%20zone', 'weapon', 'weapons',\n",
       "       'whirlwind', 'wild%20fires', 'wildfire', 'windstorm', 'wounded',\n",
       "       'wounds', 'wreck', 'wreckage', 'wrecked'], dtype=object)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['keyword'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " 'Birmingham',\n",
       " 'Est. September 2012 - Bristol',\n",
       " 'AFRICA',\n",
       " 'Philadelphia, PA',\n",
       " 'London, UK',\n",
       " 'Pretoria',\n",
       " 'World Wide!!',\n",
       " 'Paranaque City',\n",
       " 'Live On Webcam',\n",
       " 'milky way',\n",
       " 'GREENSBORO,NORTH CAROLINA',\n",
       " 'England.',\n",
       " 'Sheffield Township, Ohio',\n",
       " 'India',\n",
       " 'Barbados',\n",
       " 'Anaheim',\n",
       " 'Abuja',\n",
       " 'USA',\n",
       " 'South Africa',\n",
       " 'Sao Paulo, Brazil',\n",
       " 'hollywoodland ',\n",
       " 'Edmonton, Alberta - Treaty 6',\n",
       " 'Inang Pamantasan',\n",
       " 'Twitter Lockout in progress',\n",
       " 'Concord, CA',\n",
       " 'Calgary, AB',\n",
       " 'San Francisco',\n",
       " 'CLVLND',\n",
       " 'Nashville, TN',\n",
       " 'Santa Clara, CA',\n",
       " 'UK',\n",
       " 'St. Louis, MO',\n",
       " 'Walker County, Alabama',\n",
       " 'Australia',\n",
       " 'North Carolina',\n",
       " 'Norf Carolina',\n",
       " 'San Mateo County, CA',\n",
       " 'Njoro, Kenya',\n",
       " \"Your Sister's Bedroom\",\n",
       " 'Arlington, TX',\n",
       " 'South Bloomfield, OH',\n",
       " 'New Hanover County, NC',\n",
       " 'Maldives',\n",
       " 'Manchester, NH',\n",
       " 'Wilmington, NC',\n",
       " 'global',\n",
       " 'Alberta | Sask. | Montana',\n",
       " 'Charlotte',\n",
       " 'Baton Rouge, LA',\n",
       " 'Hagerstown, MD',\n",
       " 'Gloucestershire , UK',\n",
       " 'Nairobi, Kenya',\n",
       " 'Instagram - @heyimginog ',\n",
       " '304',\n",
       " 'Switzerland',\n",
       " 'US',\n",
       " 'Somewhere Only We Know ?',\n",
       " 'Belgium',\n",
       " 'dope show',\n",
       " 'Oshawa, Canada',\n",
       " 'Baker City Oregon',\n",
       " 'United States',\n",
       " 'marysville ca ',\n",
       " 'Hermosa Beach, CA',\n",
       " '19.600858, -99.047821',\n",
       " 'Pennsylvania',\n",
       " 'Salt Lake City, Utah',\n",
       " 'Palo Alto, CA',\n",
       " 'Spain but Opa-Locka, FL',\n",
       " 'Jaipur, India',\n",
       " 'Hyderabad Telangana INDIA',\n",
       " 'Eagle Pass, Texas',\n",
       " 'bangalore',\n",
       " 'Financial News and Views',\n",
       " 'Indonesia',\n",
       " 'y(our) boyfriends legs ',\n",
       " 'New Mexico, USA',\n",
       " 'Somewhere Out There',\n",
       " 'Mumbai india',\n",
       " 'sri lanka',\n",
       " 'Not a U.S resident',\n",
       " 'Lehigh Valley, PA',\n",
       " 'Canada',\n",
       " 'Thrissur',\n",
       " 'Havenford',\n",
       " '92',\n",
       " 'Israel',\n",
       " 'Fashion Heaven. IG: TMId_',\n",
       " 'San Francisco, CA',\n",
       " 'italy',\n",
       " 'nyc',\n",
       " 'Toronto',\n",
       " 'Jackson',\n",
       " 'New York / Worldwide',\n",
       " 'New Orleans, LA',\n",
       " 'West Wales',\n",
       " 'Happily Married with 2 kids ',\n",
       " 'Cambridge, MA',\n",
       " 'Arizona ',\n",
       " 'Mumbai',\n",
       " 'Amsterdam',\n",
       " 'Swindon,England ',\n",
       " 'Williamstown, VT',\n",
       " 'North Carolina, USA',\n",
       " 'Karachi',\n",
       " 'Loveland Colorado',\n",
       " '|| c h i c a g o ||',\n",
       " 'L. A.',\n",
       " 'VISIT MY YOUTUBE CHANNEL.',\n",
       " 'Lexington',\n",
       " 'Hannover, Germany',\n",
       " 'Playa',\n",
       " 'Davidson, NC',\n",
       " 'Higher Places',\n",
       " 'Horsemind, MI',\n",
       " 'New York, NY',\n",
       " 'Boksburg',\n",
       " 'V-RP @OZRP_ ?MV, AU, R18+?',\n",
       " 'Greater Manchester, UK',\n",
       " 'Boston',\n",
       " 'The Canopy Kingdom',\n",
       " 'the own zone layer ',\n",
       " 'London',\n",
       " 'Trancy Manor',\n",
       " 'South 37',\n",
       " 'West Lancashire, UK.',\n",
       " 'PA',\n",
       " '\\x89Û¢ Views From The Six \\x89Û¢',\n",
       " 'University of Toronto',\n",
       " 'Swaning Around',\n",
       " 'Albany/NY',\n",
       " 'California, USA',\n",
       " 'Wild Wild Web',\n",
       " 'Subconscious LA',\n",
       " 'Spain',\n",
       " 'CA physically- Boston Strong?',\n",
       " 'Yeezy Taught Me , NV',\n",
       " 'Rock Hill, SC',\n",
       " 'Coolidge, AZ',\n",
       " 'Republic of Texas',\n",
       " 'Phoenix, AZ',\n",
       " 'Ljubljana, Slovenia',\n",
       " 'Connecticut',\n",
       " 'Tacoma,Washington',\n",
       " 'BIG D  HOUSTON/BOSTON/DENVER',\n",
       " 'Chandler, AZ',\n",
       " 'ColoRADo',\n",
       " 'sindria',\n",
       " 'Texas',\n",
       " 'Elk Grove, CA, USA',\n",
       " 'The Shire',\n",
       " 'Austin, TX',\n",
       " 'Oakland',\n",
       " 'Albuquerque',\n",
       " 'Buenos Aires, Argentina',\n",
       " 'San Antonio-ish, TX',\n",
       " 'Oregon, USA',\n",
       " 'Harlingen, TX',\n",
       " 'Buffalo NY',\n",
       " 'Las Vegas',\n",
       " 'Tokyo',\n",
       " 'California, United States',\n",
       " '#FLIGHTCITY UK  ',\n",
       " 'Alphen aan den Rijn, Holland',\n",
       " 'Wrigley Field',\n",
       " 'probably the strip club',\n",
       " 'England',\n",
       " 'New York City',\n",
       " 'Here And There',\n",
       " 'Rotterdam, Zuid-Holland',\n",
       " 'Derry, 17 ',\n",
       " 'Nowhere. Everywhere.',\n",
       " 'Florida, USA',\n",
       " 'Worldwide',\n",
       " 'East Coast',\n",
       " 'California',\n",
       " 'Toronto, ON',\n",
       " 'The Orwellion police-state',\n",
       " 'Castaic, CA',\n",
       " 'Helsinki, Finland',\n",
       " 'East Kilbride',\n",
       " 'middle eastern palace',\n",
       " 'Kent',\n",
       " 'Perthshire ',\n",
       " 'twitch.tv/naturalemblem26',\n",
       " 'cyprus',\n",
       " 'Memphis, TN',\n",
       " 'Studio',\n",
       " 'Hollywood, CA',\n",
       " 'New York',\n",
       " 'Pakistan',\n",
       " 'Mexico! ^_^',\n",
       " 'Campinas Sp',\n",
       " 'Harlem, New York',\n",
       " 'Washington, DC',\n",
       " 'Burbank,CA',\n",
       " '? ',\n",
       " 'Spokane, Washington',\n",
       " 'Charlotte, NC',\n",
       " 'Our Empire State',\n",
       " 'Jerusalem',\n",
       " 'Kingston, Pennsylvania',\n",
       " 'Milwaukee, WI',\n",
       " 'Zero Branco',\n",
       " 'bajaur',\n",
       " 'Eldoret, kenya',\n",
       " '\\x89ÛÊ\\x89ÛÊ\\x89ÛÊ',\n",
       " 'Miami,FL',\n",
       " 'Los Angeles, CA',\n",
       " 'North-East Region, Singapore',\n",
       " 'Chicago',\n",
       " 'EARTH ',\n",
       " 'Jerusalem, Israel',\n",
       " 'Menasha, WI',\n",
       " 'ss',\n",
       " 'Atlanta',\n",
       " 'Earth',\n",
       " 'Bleak House',\n",
       " 'heccfidmss@gmail.com',\n",
       " 'toronto',\n",
       " '[ Blonde Bi Fry. ]',\n",
       " 'America',\n",
       " 'NYC :) Ex- #Islamophobe',\n",
       " 'SF Bay Area',\n",
       " 'Orange County, California',\n",
       " 'Winston Salem, North Carolina',\n",
       " 'Adelaide, South Australia',\n",
       " 'WASHINGTON,DC',\n",
       " ' snapchat // fvck_casper ',\n",
       " 'Dallas, TX',\n",
       " 'NYC',\n",
       " 'Location',\n",
       " 'Selena | Britney | Hilary',\n",
       " 'Ireland',\n",
       " 'Freeport IL. USA',\n",
       " 'Dubai',\n",
       " 'Tucson, Az',\n",
       " 'Seattle WA',\n",
       " 'Bellevue NE',\n",
       " 'West Bank, Gaza Strip',\n",
       " '\\x89Û¢FLG\\x89Û¢',\n",
       " 'Scotland, United Kingdom',\n",
       " 'Online 24/7. Not even kidding.',\n",
       " 'rowyso dallas ',\n",
       " 'Halton, Ontario',\n",
       " 'portland, oregon',\n",
       " 'FIMAK A.S Ist Bolge Muduru',\n",
       " 'London.',\n",
       " '#UNITE THE BLUE  ',\n",
       " 'Dayton, Ohio',\n",
       " 'Global',\n",
       " 'ph',\n",
       " 'Port Jervis, NY',\n",
       " 'City Of Joy',\n",
       " 'Texas, USA',\n",
       " '1/3 of the blam squad ',\n",
       " 'CCH ',\n",
       " 'atx',\n",
       " 'MAURITIUS',\n",
       " 'AKRON OHIO USA',\n",
       " 'london / st catharines ?',\n",
       " 'Peshawar',\n",
       " 'LEALMAN, FLORIDA',\n",
       " '#GDJB #ASOT',\n",
       " 'Groningen, Netherlands, Europe',\n",
       " 'Livingston, IL  U.S.A.',\n",
       " 'Arundel ',\n",
       " 'Anna Maria, FL',\n",
       " 'israel',\n",
       " 'The Hammock, FL, USA',\n",
       " '??????????????????',\n",
       " 'SÌ£o Paulo SP,  Brasil',\n",
       " \"in Dimitri's arms\",\n",
       " 'Oslo, Norway',\n",
       " 'Los Angeles',\n",
       " 'Loughton, Essex, UK',\n",
       " 'guaravitas',\n",
       " 'Score More Goals Buying @',\n",
       " 'NEW YORK',\n",
       " 'London, Kent & SE England.',\n",
       " 'Score Team Goals Buying @',\n",
       " 'South Central Wales',\n",
       " 'Jersey City, New Jersey',\n",
       " 'Denver, CO',\n",
       " 'Brasil',\n",
       " 'Buy Give Me My Money ',\n",
       " '505 W. Maple, Suite 100',\n",
       " 'Philippines',\n",
       " 'Freeport il ',\n",
       " 'World',\n",
       " 'Danville, VA',\n",
       " 'UK Great Britain ',\n",
       " 'Jerusalem!',\n",
       " 'San Jose, CA',\n",
       " 'Use #TMW in tweets get #RT',\n",
       " 'Utah',\n",
       " 'Wisconsin',\n",
       " 'West Richland, WA',\n",
       " 'CHICAGO (312)',\n",
       " 'Washington D.C.',\n",
       " 'NC',\n",
       " 'West Virginia, USA',\n",
       " 'British girl in Texas',\n",
       " 'Silver Spring, MD',\n",
       " 'Atlanta, GA',\n",
       " 'Manhattan, NY',\n",
       " 'Wilmington, DE',\n",
       " 'Memphis',\n",
       " 'iTunes',\n",
       " 'Oxford, MS',\n",
       " 'Pelham, AL',\n",
       " 'Jacksonville, FL',\n",
       " 'Arkansas, Jonesboro',\n",
       " 'Across the Atlantic',\n",
       " 'Melbourne, Florida',\n",
       " 'Over the Moon...',\n",
       " 'Extraterrestrial Highway',\n",
       " 'Espoo, Finland',\n",
       " 'Washington, D.C., area',\n",
       " 'OES 4th Point. sisSTAR & TI',\n",
       " 'Sydney, New South Wales',\n",
       " 'Netherlands,Amsterdam-Virtual ',\n",
       " 'Hudson Valley, NY',\n",
       " 'timeline kamu',\n",
       " 'Budapest, Hungary',\n",
       " 'Searching for Bae ',\n",
       " 'Eagle Mountain, Texas ',\n",
       " 'Columbus',\n",
       " 'Temecula, CA',\n",
       " 'Atlanta,Ga',\n",
       " 'Fresno, CA',\n",
       " 'Raleigh Durham, NC',\n",
       " 'ducked off . . . ',\n",
       " 'Rio de Janeiro',\n",
       " '302',\n",
       " 'Columbus, OH',\n",
       " 'Mo.City',\n",
       " 'Tripsburg, ms.',\n",
       " 'Durham N.C ',\n",
       " 'Delhi',\n",
       " 'ARIZONA',\n",
       " 'Penn Hills, PA',\n",
       " 'Karachi ',\n",
       " 'seattle wa',\n",
       " 'SOUTHERN CALIFORNIA DESERT',\n",
       " 'Gotham City',\n",
       " 'My contac 27B80F7E 08170156520',\n",
       " 'Pig Symbol, Alabama',\n",
       " 'Intramuros, Manila',\n",
       " 'THE WORLD T.G.G / M.M.M ',\n",
       " 'Between the worlds ',\n",
       " 'State of Georgia',\n",
       " 'Your screen',\n",
       " 'Lima-Peru',\n",
       " 'worldwide',\n",
       " 'Saltillo, Coahuila de Zaragoza',\n",
       " 'Suitland',\n",
       " 'Everywhere',\n",
       " 'Konoha',\n",
       " 'Swag Francisco',\n",
       " 'Saint Marys, GA',\n",
       " 'Nigeria',\n",
       " 'Essex/Brighton',\n",
       " 'Pennsylvania, PA',\n",
       " 'Rockford, IL',\n",
       " 'AZ',\n",
       " 'Vero Beach , FL',\n",
       " 'IN',\n",
       " 'In the middle of no where',\n",
       " 'Baltimore, MD',\n",
       " 'dmv ?? fashion school @ KSU. ',\n",
       " 'Nice places ',\n",
       " 'Quantico, VA',\n",
       " 'Island Lake, IL',\n",
       " 'Live Oak, TX',\n",
       " 'Gages Lake, IL',\n",
       " 'Madisonville TN',\n",
       " 'Basketball City, USA ',\n",
       " 'AEP',\n",
       " 'Alberta Pack',\n",
       " '#expelcl*y',\n",
       " 'My heart is a ghost town!',\n",
       " 'The Great State of Texas',\n",
       " 'Alicante, Valencia',\n",
       " 'Greensboro, NC',\n",
       " ' Indiana',\n",
       " 'the local dump',\n",
       " 'Seattle',\n",
       " '#SOUTHAMPTON ENGLAND',\n",
       " '?205?478?',\n",
       " 'Waterford MI',\n",
       " 'Iowa, USA',\n",
       " 'va',\n",
       " 'Florida',\n",
       " 'california mermaid ? ',\n",
       " 'New York ? ATL',\n",
       " 'USA/SO FLORIDA via BROOKLYN NY',\n",
       " 'H / pez & sophia ',\n",
       " 'Brooklyn, NY',\n",
       " 'Queens, NY',\n",
       " 'Vancouver, BC.',\n",
       " 'Ottawa, Canada',\n",
       " 'wherever the $$$ at',\n",
       " 'Purgatory, USA',\n",
       " 'Calgary, Alberta',\n",
       " 'Kama | 18 | France ',\n",
       " 'Sydney',\n",
       " 'Maryland, USA',\n",
       " 'Daruka (near Tamworth) NSW',\n",
       " 'IJmuiden, The Netherlands',\n",
       " 'University Heights, Ohio',\n",
       " 'Central Illinois',\n",
       " 'Baton Rouge',\n",
       " 'PSN: Pipbois ',\n",
       " 'Colombo,Sri Lanka.',\n",
       " 'Johannesburg, South Africa ',\n",
       " \"Me mammy's belly\",\n",
       " 'UK & Ibiza',\n",
       " 'Laventillemoorings ',\n",
       " 'Scotland',\n",
       " 'Vancouver, BC',\n",
       " 'Cleveland, OH',\n",
       " 'Detroit, MI, United States',\n",
       " 'Guelph Ontario Canada',\n",
       " 'Waterfront',\n",
       " 'columbus ohio',\n",
       " 'Waukesha, WI',\n",
       " 'Himalayan Mountains',\n",
       " 'Colorado/WorldWide',\n",
       " 'Ontario Canada',\n",
       " 'Houston,  TX',\n",
       " 'lakewood colorado',\n",
       " 'THE 6IX',\n",
       " 'Washington, USA',\n",
       " 'The ?? below ???',\n",
       " 'Ideally under a big tree',\n",
       " \"Conversing In Janet's CafÌ¬\",\n",
       " \"???????, ??'??????\",\n",
       " \"Dime's Palace\",\n",
       " 'Cairo, Egypt',\n",
       " 'Seattle, WA',\n",
       " 'Bug Forest',\n",
       " 'canberra',\n",
       " 'International',\n",
       " 'The World',\n",
       " 'Htx',\n",
       " 'PunPunlÌ¢ndia',\n",
       " '???',\n",
       " 'Itirapina, SÌ£o Paulo',\n",
       " 'North Jersey',\n",
       " 'Ewa Beach, HI',\n",
       " 'Biloxi, Mississippi',\n",
       " 'Buenos Aires',\n",
       " 'Brecksville, OH',\n",
       " 'Walthamstow, London',\n",
       " 'Malaysia',\n",
       " 'Ivano-Frankivsk',\n",
       " 'Sunshine Coast, Queensland',\n",
       " 'England,UK,Europe,Sol 3.',\n",
       " 'Nashua NH',\n",
       " 'Leicester',\n",
       " '65',\n",
       " 'Westchester',\n",
       " 'Lynnfield, MA',\n",
       " 'Level 3 Garrison, Sector G',\n",
       " 'Singapore',\n",
       " 'Glasgow',\n",
       " 'Shity land of Northern Ireland',\n",
       " 'Christchurch New Zealand',\n",
       " 'peekskill. new york, 10566 ',\n",
       " 'PH',\n",
       " 'Storybrooke ',\n",
       " 'under the blanket',\n",
       " 'Isolated City In World Perth',\n",
       " 'Brazil ',\n",
       " 'AUS',\n",
       " 'Santa Cruz, CA',\n",
       " 'Inverness, Nova Scotia',\n",
       " 'L/S/Z/L/T/H/C/H/R/A/S/C',\n",
       " 'Oklahoma',\n",
       " '801 SL,UT',\n",
       " 'fluffy cloud',\n",
       " 'The 5th Dimension. ',\n",
       " 'Georgia',\n",
       " 'July 11th, 2015. ?',\n",
       " 'The Grey Area',\n",
       " 'St Paul, MN',\n",
       " 'Cobblestone',\n",
       " 'ÌÏT: 30.307558,-81.403118',\n",
       " 'Cosmic Oneness',\n",
       " 'Guildford, UK',\n",
       " 'Nowhere Islands/Smash Manor',\n",
       " 'kisumu',\n",
       " 'Making Worldwide Change Near U',\n",
       " 'ATX',\n",
       " 'LA/OC/Vegas',\n",
       " 'London/Outlaw Country ',\n",
       " 'Grimsby, England',\n",
       " 'Gotham',\n",
       " 'Manchester, The World, England',\n",
       " 'sitting on Eddie Vedders lap,',\n",
       " 'Missouri, USA',\n",
       " 'Greenville,SC',\n",
       " 'Paignton',\n",
       " 'have car; will travel',\n",
       " 'ATL ? SEA ',\n",
       " 'ÌÏT: 39.982988,-75.261624',\n",
       " '#EngleWood CHICAGO ',\n",
       " '302???? 815',\n",
       " 'New Your',\n",
       " 'PURPLE BOOTH STUDIO\\x89ã¢',\n",
       " 'Cloud 9',\n",
       " 'Former Yugoslav Republic of Macedonia',\n",
       " '3?3?7?SLOPelousas??2?2?5?',\n",
       " '316',\n",
       " \"#WhereverI'mAt\",\n",
       " 'Huber Heights, OH',\n",
       " 'Miami ??',\n",
       " 'Houston, TX',\n",
       " 'Every where',\n",
       " \"401 livin'\",\n",
       " 'MI',\n",
       " 'EPTX',\n",
       " 'Austin, Texas',\n",
       " 'Oklahoma City',\n",
       " 'CA',\n",
       " 'United Kingdom',\n",
       " 'WESTSIDE OF PHILLY 7? BLOCK??',\n",
       " 'Charlotte NC',\n",
       " 'LONG ISLAND, NY',\n",
       " 'washington, d.c.',\n",
       " 'WAISTDEEP, TX',\n",
       " 'D.C. - Baltimore - Annapolis',\n",
       " '#937??#734',\n",
       " 'Fife, WA',\n",
       " 'Bushkill pa',\n",
       " 'In the Shadows...',\n",
       " 'southwest, Tx',\n",
       " 'ANYWEHERE !!',\n",
       " 'Menlo Park. SFO. The World.',\n",
       " 'Speaking the Truth in Love',\n",
       " 'Aarhus, Central Jutland',\n",
       " 'Lincoln, NE',\n",
       " 'wny',\n",
       " 'Bolton & Tewkesbury, UK',\n",
       " \"travelling to tae's pants\",\n",
       " 'keli x',\n",
       " 'Manchester',\n",
       " 'Knoxville, TN',\n",
       " 'ChicagoRObotz',\n",
       " \"whs '17\",\n",
       " 'NV',\n",
       " 'lagos nigeria',\n",
       " 'Edmonton, Alberta',\n",
       " '[Gia.] | #KardashianEmpire',\n",
       " 'Sunrise Manor, NV',\n",
       " 'Oxford, OH',\n",
       " 'Odawara, Japan',\n",
       " 'Chicago, IL',\n",
       " 'Des Moines, IA',\n",
       " 'Dundas, Ontario',\n",
       " 'IDN',\n",
       " 'Netherlands',\n",
       " 'DaKounty, Pa',\n",
       " 'DÌ_sseldorf, Germany',\n",
       " 'Old Blighty',\n",
       " '??',\n",
       " 'The land of New Jersey. ',\n",
       " 'Atlanta Georgia',\n",
       " 'MY RTs ARE NOT ENDORSEMENTS',\n",
       " 'Kabul, Tuebingen, Innsbruck',\n",
       " 'Light and dark, form and void',\n",
       " 'Nairobi, Kenya ',\n",
       " 'Erbil',\n",
       " 'Stockton on tees Teesside UK',\n",
       " 'Screwston, TX',\n",
       " 'My old New England home',\n",
       " 'melbourne',\n",
       " 'Cape Town',\n",
       " 'Ikeja, Nigeria',\n",
       " 'Warwick, RI @Dollarocracy also',\n",
       " 'texas a&m university',\n",
       " 'MA',\n",
       " 'Shipwreck Cove',\n",
       " 'Sydney, Australia',\n",
       " 'WorldWide',\n",
       " 'Overland Park, KS',\n",
       " 'SWMO',\n",
       " 'Puerto Rico',\n",
       " 'Toronto-Citizen of Canada & US',\n",
       " 'Silicon Valley',\n",
       " 'North East USA',\n",
       " 'Washington, D.C.',\n",
       " 'VitÌ_ria (ES)',\n",
       " 'New Delhi, Delhi',\n",
       " 'Buscame EL tu Melte',\n",
       " 'Wiltshire',\n",
       " 'PROUD INDIANS',\n",
       " 'Mumbai , India',\n",
       " 'Pittsburgh PA',\n",
       " 'MUM-DEL',\n",
       " 'Leeds, England',\n",
       " 'Playa del Carmen, Mexico',\n",
       " 'NY, CT & Greece',\n",
       " 'Mackay, QLD, Australia',\n",
       " 'Quincy MA',\n",
       " 'NJ',\n",
       " 'Madison, GA',\n",
       " 'KurveZ@GearHeadCentral.net',\n",
       " 'St Charles, MD',\n",
       " 'Denver, Colorado',\n",
       " 'Ziam af ',\n",
       " 'GO BLUE! HAIL YES!!',\n",
       " 'Outside The Matrix, I Think.',\n",
       " 'In Hell',\n",
       " 'Epic City, BB.',\n",
       " 'somewhere over a rainbow',\n",
       " 'Selma2Oakland',\n",
       " 'Bombardment Bay',\n",
       " 'Savannah, GA',\n",
       " 'dallas',\n",
       " 'New Orleans ,Louisiana',\n",
       " 'England, United Kingdom',\n",
       " 'Dublin City, Ireland',\n",
       " 'New Hampshire',\n",
       " 'World Wide',\n",
       " 'Intermountain West',\n",
       " 'Tulsa, Oklahoma',\n",
       " 'Auburn, AL',\n",
       " 'taken by piper curda',\n",
       " 'Scotland ',\n",
       " 'Nigeria, Global',\n",
       " 'Fort Walton Beach, Fl',\n",
       " 'Sweden',\n",
       " 'Groton, CT',\n",
       " 'Brisbane Australia',\n",
       " 'NY Capital District',\n",
       " 'Peoria',\n",
       " 'Reading UK',\n",
       " 'Concord, NH ',\n",
       " 'CORNFIELDS',\n",
       " 'Worcester, MA',\n",
       " 'Roanoke, VA',\n",
       " 'nj',\n",
       " 'MA via PA',\n",
       " 'Boston, MA',\n",
       " 'Oklahoma City, OK',\n",
       " 'toronto \\x89Û¢ dallas',\n",
       " 'San Diego CA',\n",
       " 'germany',\n",
       " 'Massachusetts',\n",
       " ' Nxgerxa',\n",
       " 'Erie, PA',\n",
       " 'Port Charlotte, FL',\n",
       " 'Belleville, Illinois',\n",
       " 'Alabama',\n",
       " 'Long Island NY & San Francisco',\n",
       " 'Gainesville, FL',\n",
       " 'Oakland, CA',\n",
       " '956',\n",
       " 'Escondido, CA',\n",
       " 'DC',\n",
       " 'Chicago Area',\n",
       " 'Upper St Clair, PA',\n",
       " 'Cherry Creek Denver CO',\n",
       " '627',\n",
       " 'Blogland',\n",
       " 'mumbai',\n",
       " 'Isle of Man',\n",
       " 'Hampton Roads, VA',\n",
       " 'Gameday',\n",
       " 'Earthling (For now!)',\n",
       " 'Black Canyon New River, AZ',\n",
       " 'Caracas, Venezuela.',\n",
       " 'NY',\n",
       " 'Charlottetown',\n",
       " 'Paradise, NV',\n",
       " '??t?a',\n",
       " 'LiÌ¬ge',\n",
       " 'Spokane, Washington 99206',\n",
       " 'taco bell',\n",
       " 'Australian Capital Territory',\n",
       " 'http://www.amazon.com/dp/B00HR',\n",
       " '[ kate + they/them + infp-t ]',\n",
       " 'Sacramento, CA',\n",
       " 'please H? ?:??',\n",
       " 'we?it \\x89Û¢ ixwin',\n",
       " 'Santiago Bernabeau',\n",
       " 'y/e/l',\n",
       " 'Whiterun, Skyrim',\n",
       " 'Greenpoint, Brooklyn',\n",
       " 'Victoria, British Columbia',\n",
       " 'a botanical garden probably',\n",
       " 'Nelspruit, South Africa',\n",
       " 'midwest',\n",
       " 'Copenhagen, Capital Region of Denmark',\n",
       " 'Spying on your thoughts',\n",
       " 'seattle grace mercy death',\n",
       " 'Tucson, Arizona ',\n",
       " 'Charlotte County Florida',\n",
       " 'Head Office: United Kingdom',\n",
       " 'Mid north coast of NSW',\n",
       " 'iPhone: -27.499212,153.011072',\n",
       " 'Queen Creek AZ',\n",
       " 'Jamaica',\n",
       " 'Trinidad and Tobago',\n",
       " 'Melbourne Australia',\n",
       " 'The Internet & NYC',\n",
       " 'London/Bristol/Guildford',\n",
       " 'Canberra, Australian Capital Territory',\n",
       " 'beacon hills ',\n",
       " 'somewhere outside',\n",
       " 'Wolmers Trust School for Boys ',\n",
       " 'Loughborough.',\n",
       " 'Selangor',\n",
       " 'indiana',\n",
       " 'Bronx, New York',\n",
       " 'Canadian bread',\n",
       " \"Le Moyne '16\",\n",
       " 'Hoxton, London',\n",
       " 'Las Vegas, NV USA',\n",
       " 'Heinz Field ',\n",
       " 'Skyport de la Rosa',\n",
       " 'The Low-Cal Calzone Zone',\n",
       " 'Brooklyn, New York',\n",
       " '50% Queanbeyan - 50% Sydney',\n",
       " 'Insula Barataria',\n",
       " 'everywhere ',\n",
       " 'Afghanistan, USA',\n",
       " 'Inglewood, CA',\n",
       " 'Absecon, NJ',\n",
       " 'Williamsbridge, Bronx, New Yor',\n",
       " 'Northern Kentucky, USA',\n",
       " 'Mostly Yuin.',\n",
       " 'Nairobi',\n",
       " 'Rochelle, GA',\n",
       " 'London UK',\n",
       " 'south of heaven ',\n",
       " 'El Dorado, KS',\n",
       " 'Santa Monica, CA',\n",
       " 'Kenya',\n",
       " '1648 Queen St. West, Toronto.',\n",
       " 'Toledo, OH',\n",
       " 'Fairfield, California',\n",
       " 'Trinity, Bailiwick of Jersey',\n",
       " 'Virginia',\n",
       " 'LIVERPOOL',\n",
       " 'Boulder, CO',\n",
       " 'Hospital, bc of SKH vid.',\n",
       " 'Hartford  London Hong Kong',\n",
       " \"In @4SkinChan 's arms\",\n",
       " 'Massachusetts, USA',\n",
       " 'Kansas City, MO',\n",
       " 'Wellington, New Zealand',\n",
       " '?? ??',\n",
       " 'los angeles, ca',\n",
       " 'New Brunswick, NJ',\n",
       " 'City of Angels, CA',\n",
       " 'Morganville, Texas.',\n",
       " 'America | New Zealand ',\n",
       " 'Stockholm, Sweden',\n",
       " 'Azeroth',\n",
       " \"Lytham St Anne's \",\n",
       " 'Ylisse',\n",
       " 'Portugal',\n",
       " 'All around the world baby',\n",
       " '@UntmdOutdoors #T.O.R.K ',\n",
       " 'Lima, PerÌ¼',\n",
       " 'Piedmont Area, North Carolina',\n",
       " 'Buxton, Venice, and Nottingham',\n",
       " 'Quito, Ecuador.',\n",
       " 'Inexpressible Island ',\n",
       " ' Bouvet Island',\n",
       " 'Shirley, NY',\n",
       " 'Planet Earth',\n",
       " 'Paonia, Colorado ',\n",
       " 'Portland, OR',\n",
       " 'Dublin, Ireland',\n",
       " 'Lurking',\n",
       " 'N?? Y???.',\n",
       " 'Littleton, CO, USA',\n",
       " 'Virginia, United States',\n",
       " 'Center for Domestic Preparedness',\n",
       " 'Ukraine and Ireland',\n",
       " 'Welt',\n",
       " 'Wales',\n",
       " 'Seattle, Washington',\n",
       " 'Beaumont, TX',\n",
       " 'Annapolis, MD',\n",
       " 'Las Vegas, Nevada',\n",
       " 'Harris County, Texas',\n",
       " 'Tyler, TX',\n",
       " 'Evanston, IL',\n",
       " 'North Ferriby, East Yorkshire',\n",
       " 'Colombia',\n",
       " 'Ames, Iowa',\n",
       " 'Moscow, Russia',\n",
       " 'Jersey City, NJ',\n",
       " 'Mankato, MN',\n",
       " 'Orbost, Victoria, Australia',\n",
       " ' Somewhere.',\n",
       " ' Neverland ',\n",
       " 'London, England',\n",
       " 'At Da Laundry Mat Wit Nivea ',\n",
       " 'norway',\n",
       " '$ad $hawty',\n",
       " 'NY || live easy? ',\n",
       " 'BestCoast',\n",
       " 'BC',\n",
       " 'Abuja, Nigeria',\n",
       " 'Colchester Essex ',\n",
       " 'livin life in the 610',\n",
       " 'Suplex City',\n",
       " 'Inside your mind.',\n",
       " '36 & 38',\n",
       " 'Sydney Australia',\n",
       " 'Lagos, Nigeria',\n",
       " 'New York, NY ',\n",
       " 'Slappin and Smackin ',\n",
       " \"DRAW A CIRCLE THAT'S THE EARTH\",\n",
       " 'Madrid, Comunidad de Madrid',\n",
       " 'The Netherlands',\n",
       " 'Highland Park, CA',\n",
       " 'Swan River',\n",
       " 'Kolkata, India',\n",
       " 'Melrose',\n",
       " 'Jubail IC, Saudi Arabia',\n",
       " 'Sugarhouse, UT',\n",
       " 'lugo',\n",
       " 'Chicago, Illinois',\n",
       " 'Blackpool, England, UK.',\n",
       " 'San Jose, California',\n",
       " 'Fakefams',\n",
       " 'In the clouds...',\n",
       " 'Sandton, South Africa',\n",
       " 'Nigeria ',\n",
       " '#Bummerville otw',\n",
       " 'Europe',\n",
       " 'Brighton and Hove',\n",
       " 'Pompano Beach, FL',\n",
       " 'JKT48-Muse-A7X',\n",
       " 'Behind The Obama Curtain',\n",
       " 'Worldwide.',\n",
       " 'LiVE MÌ\\x81S',\n",
       " 'Henderson, NV',\n",
       " 'CAMARILLO, CA',\n",
       " 'manchester, uk.',\n",
       " 'Eugene, Oregon',\n",
       " 'they/her',\n",
       " 'Paris ',\n",
       " '(he/him)',\n",
       " 'Kingston, Jamaica',\n",
       " 'Spokane, WA',\n",
       " \"I'm standing behind you\",\n",
       " 'Alexandria, Egypt.',\n",
       " 'instagram- Chloe_Bellx',\n",
       " 'on the web',\n",
       " 'San Francisco Bay Area',\n",
       " '#ForeverWithBAP 8 ',\n",
       " 'GOT7SupportPH',\n",
       " 'wherever-the-fuck washington',\n",
       " 'From NY. In Scranton, PA',\n",
       " 'Perth, Western Australia',\n",
       " 'Live mÌÁs',\n",
       " 'Melton, GA',\n",
       " \"Greg's place\",\n",
       " 'Viejo',\n",
       " 'Kansas, The Free State! ~ KC',\n",
       " 'Silang, Cavite / ParaÌ±aque',\n",
       " 'Fort Smith, AR',\n",
       " 'planeta H2o',\n",
       " 'www.youtube.com?Malkavius2',\n",
       " 'Michigan',\n",
       " 'The Forever Girl',\n",
       " 'EspÌ_rito Santo',\n",
       " 'New York, USA',\n",
       " 'Ormond By The Sea, FL',\n",
       " 'Maryland,Baltimore',\n",
       " 'Pennsylvania, USA',\n",
       " 'Houston, Texas',\n",
       " 'Yadkinville, NC',\n",
       " 'Medford, NJ',\n",
       " 'Dallas, Texas. ',\n",
       " 'Roanoke VA',\n",
       " 'Unnamed City',\n",
       " 'Peterborough, On',\n",
       " '-6.152261,106.775995',\n",
       " \"@protectingtitan's side.\",\n",
       " 'Oregon',\n",
       " 'Eau Claire, Wisconsin',\n",
       " 'St. Joseph, Minnesota',\n",
       " 'From a torn up town MANCHESTER',\n",
       " 'Waco TX',\n",
       " 'Tennessee',\n",
       " 'Cape Cod, Massachusetts USA',\n",
       " 'bk. ',\n",
       " 'Lansing, Michigan',\n",
       " 'Peterborough, Ont.',\n",
       " 'Johannesburg, South Africa',\n",
       " 'On the court ',\n",
       " 'Traverse City, MI',\n",
       " 'See the barn of bleakness',\n",
       " 'Hertfordshire ',\n",
       " 'ÌÏT: 41.252426,-96.072013',\n",
       " 'California ',\n",
       " 'Greeley, CO',\n",
       " \"'soooota\",\n",
       " 'Sacramento',\n",
       " 'Ontario, Canada',\n",
       " 'NY, NY',\n",
       " 'Arvada, CO',\n",
       " 'Maryland',\n",
       " 'Vancouver, Colombie-Britannique',\n",
       " 'Irving , Texas',\n",
       " 'Riverside, CA',\n",
       " 'SEATTLE, WA USA',\n",
       " 'USA , AZ',\n",
       " 'East Atlanta, Georgia',\n",
       " 'Saint Lucia',\n",
       " 'Peterborough, Ontario, Canada',\n",
       " 'Colorado',\n",
       " 'North Highlands, CA',\n",
       " 'Vancouver',\n",
       " '60th St (SS)',\n",
       " 'USA, WA',\n",
       " 'btwn a rock and a hard place',\n",
       " 'Aix-en-Provence, France',\n",
       " 'Liverpool',\n",
       " 'I-75 in Florida',\n",
       " '21.462446,-158.022017',\n",
       " 'Melbourne, Australia',\n",
       " 'Darlington',\n",
       " '  Melbourne, Australia',\n",
       " 'Kenton, Ohio',\n",
       " 'Galatians 2:20 ',\n",
       " 'Charleston, SC',\n",
       " 'Lancaster, Pennsylvania, USA',\n",
       " 'In my own world!!!',\n",
       " 'Definitely NOT the stables',\n",
       " 'Cuernavaca, Morelos, MÌ©xico.',\n",
       " 'Va Beach, Virginia',\n",
       " '52.479722, 62.184971',\n",
       " 'The Pig Sty',\n",
       " 'Bangor, Co.Down',\n",
       " 'Weston super mare',\n",
       " 'Victoria, Tx.',\n",
       " 'Pakistan, Islamabad',\n",
       " 'Gujranwala, Pakistan',\n",
       " 'Lindenhurst',\n",
       " 'Islamabad',\n",
       " 'Somewhere',\n",
       " 'too far',\n",
       " 'Kingswinford',\n",
       " 'i love the smurfs 2',\n",
       " 'International ',\n",
       " \"Viterbo BFA Acting '18\",\n",
       " 'Kaneohe',\n",
       " 'Houma La',\n",
       " \"EastAtlanta ??#WestGeorgia'18\",\n",
       " 'San Antonio, TX',\n",
       " 'Cleveland, Ohio',\n",
       " 'San Diego, Texas.',\n",
       " 'GLOBAL',\n",
       " '05/04/2014 18:23 ?',\n",
       " 'Bolivar, MO',\n",
       " 'taking pain like pleasure',\n",
       " 'San Fransokyo',\n",
       " 'Washington, DC NATIVE',\n",
       " 'Miami, FL',\n",
       " 'w. Nykae ',\n",
       " 'honeymoon avenue',\n",
       " 'Toronto, Worldwide ',\n",
       " '11/4/14',\n",
       " \"wherever there's netflix\",\n",
       " '#HAMont',\n",
       " 'U.K.',\n",
       " 'online ',\n",
       " 'Conroe, TX',\n",
       " 'Ontario, Canada. ',\n",
       " 'Guayaquil',\n",
       " 'bahstun/porta reeko',\n",
       " 'Bucks County, Pa',\n",
       " 'Sunny South florida ',\n",
       " 'Rio',\n",
       " 'Neverland',\n",
       " 'Trinidad & Tobago',\n",
       " 'Chicago - Lake Buena Vista',\n",
       " 'Toronto, Ontario',\n",
       " 'Liberty Lake, WA',\n",
       " 'Ankara - Malatya - ad Orontem',\n",
       " 'antioch, california',\n",
       " 'State College, PA',\n",
       " 'somewhere in cali ',\n",
       " 'Im Around ... Jersey',\n",
       " 'Garden Grove',\n",
       " 'Adelaide, Australia',\n",
       " 'BKI-KUA',\n",
       " 'urÌ£nus',\n",
       " 'Illinois, USA',\n",
       " ...]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data_train['location'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all',\n",
       "       'Forest fire near La Ronge Sask. Canada',\n",
       "       \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\",\n",
       "       ...,\n",
       "       'M1.94 [01:04 UTC]?5km S of Volcano Hawaii. http://t.co/zDtoyd8EbJ',\n",
       "       'Police investigating after an e-bike collided with a car in Little Portugal. E-bike rider suffered serious non-life threatening injuries.',\n",
       "       'The Latest: More Homes Razed by Northern California Wildfire - ABC News http://t.co/YmY4rSkQ3d'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[data_train['target']==1].text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7582</th>\n",
       "      <td>10834</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cramer: Iger's 3 words that wrecked Disney's s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7584</th>\n",
       "      <td>10837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These boxes are ready to explode! Exploding Ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7587</th>\n",
       "      <td>10841</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sirens everywhere!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7593</th>\n",
       "      <td>10848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just heard a really loud bang and everyone i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4342 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword location  \\\n",
       "15       23      NaN      NaN   \n",
       "16       24      NaN      NaN   \n",
       "17       25      NaN      NaN   \n",
       "18       26      NaN      NaN   \n",
       "19       28      NaN      NaN   \n",
       "...     ...      ...      ...   \n",
       "7581  10833  wrecked  Lincoln   \n",
       "7582  10834  wrecked      NaN   \n",
       "7584  10837      NaN      NaN   \n",
       "7587  10841      NaN      NaN   \n",
       "7593  10848      NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "15                                       What's up man?       0  \n",
       "16                                        I love fruits       0  \n",
       "17                                     Summer is lovely       0  \n",
       "18                                    My car is so fast       0  \n",
       "19                         What a goooooooaaaaaal!!!!!!       0  \n",
       "...                                                 ...     ...  \n",
       "7581  @engineshed Great atmosphere at the British Li...       0  \n",
       "7582  Cramer: Iger's 3 words that wrecked Disney's s...       0  \n",
       "7584  These boxes are ready to explode! Exploding Ki...       0  \n",
       "7587                                 Sirens everywhere!       0  \n",
       "7593  I just heard a really loud bang and everyone i...       0  \n",
       "\n",
       "[4342 rows x 5 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[data_train['target']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(STOPWORDS))\n",
    "tt=TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm=[]\n",
    "for i in stopwords:\n",
    "    if i in rm:\n",
    "        stopwords.remove(i)\n",
    "pun=list(punctuation)\n",
    "\n",
    "not_list=pun+stopwords\n",
    "\n",
    "\n",
    "def clean(x):\n",
    "    tokens=tt.tokenize(x.lower())\n",
    "    return \" \".join([j for j in tokens if j not in not_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['clean_text']=data_train['text'].apply(lambda x:(clean(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \n",
       "0       1      deeds reason #earthquake may allah forgive us  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  residents asked shelter place notified officer...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 6)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['clean']=data_test['text'].apply(lambda x:clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard #earthquake different cities stay safe e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting #spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                               clean  \n",
       "0                        happened terrible car crash  \n",
       "1  heard #earthquake different cities stay safe e...  \n",
       "2  forest fire spot pond geese fleeing across str...  \n",
       "3            apocalypse lighting #spokane #wildfires  \n",
       "4             typhoon soudelor kills 28 china taiwan  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect=CountVectorizer(max_features=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_train.clean_text.values\n",
    "Y=data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vect=TfidfVectorizer(max_features=1200)\n",
    "x_train_bow=count_vect.fit_transform(X_train)\n",
    "x_test_bow=count_vect.transform(X_test)\n",
    "x_ttest_bow=count_vect.transform(data_test.clean.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851, 1200) (762, 1200)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_bow.shape,x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 : 76.50918635170603\n",
      "0.2 : 76.24671916010499\n",
      "0.01 : 76.50918635170603\n",
      "0.5 : 75.8530183727034\n",
      "0.05 : 76.50918635170603\n",
      "1 : 76.50918635170603\n",
      "1.5 : 77.03412073490814\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alpa=[0.1,0.2,0.01,0.5,0.05,1,1.5]\n",
    "for i in alpa:\n",
    "    nb=MultinomialNB(alpha=i)\n",
    "    nb.fit(x_train_bow,y_train)\n",
    "    print(i,\":\",accuracy_score(y_test,nb.predict(x_test_bow))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.11548556430446"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "sv=LinearSVC()\n",
    "sv.fit(x_train_bow,y_train)\n",
    "accuracy_score(y_test,sv.predict(x_test_bow))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(nb.predict(x_ttest_bow)).to_csv('pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every keyword in train data is in test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets clean the data more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean1(tweet): \n",
    "            \n",
    "    # Special characters\n",
    "    tweet = re.sub(r\"\\x89Û_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÒ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÓ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏWhen\", \"When\", tweet)\n",
    "    tweet = re.sub(r\"\\x89ÛÏ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"China\\x89Ûªs\", \"China's\", tweet)\n",
    "    tweet = re.sub(r\"let\\x89Ûªs\", \"let's\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û÷\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Ûª\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û\\x9d\", \"\", tweet)\n",
    "    tweet = re.sub(r\"å_\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢\", \"\", tweet)\n",
    "    tweet = re.sub(r\"\\x89Û¢åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"fromåÊwounds\", \"from wounds\", tweet)\n",
    "    tweet = re.sub(r\"åÊ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"åÈ\", \"\", tweet)\n",
    "    tweet = re.sub(r\"JapÌ_n\", \"Japan\", tweet)    \n",
    "    tweet = re.sub(r\"Ì©\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"å¨\", \"\", tweet)\n",
    "    tweet = re.sub(r\"SuruÌ¤\", \"Suruc\", tweet)\n",
    "    tweet = re.sub(r\"åÇ\", \"\", tweet)\n",
    "    \n",
    "    # Contractions\n",
    "    tweet = re.sub(r\"he's\", \"he is\", tweet)\n",
    "    tweet = re.sub(r\"there's\", \"there is\", tweet)\n",
    "    tweet = re.sub(r\"We're\", \"We are\", tweet)\n",
    "    tweet = re.sub(r\"That's\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"won't\", \"will not\", tweet)\n",
    "    tweet = re.sub(r\"they're\", \"they are\", tweet)\n",
    "    tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n",
    "    tweet = re.sub(r\"don\\x89Ûªt\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"aren't\", \"are not\", tweet)\n",
    "    tweet = re.sub(r\"isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"What's\", \"What is\", tweet)\n",
    "    tweet = re.sub(r\"haven't\", \"have not\", tweet)\n",
    "    tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n",
    "    tweet = re.sub(r\"There's\", \"There is\", tweet)\n",
    "    tweet = re.sub(r\"He's\", \"He is\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"You're\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"I'M\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n",
    "    tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"i'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"I'm\", \"I am\", tweet)\n",
    "    tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n",
    "    tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"you've\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªve\", \"you have\", tweet)\n",
    "    tweet = re.sub(r\"we're\", \"we are\", tweet)\n",
    "    tweet = re.sub(r\"what's\", \"what is\", tweet)\n",
    "    tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n",
    "    tweet = re.sub(r\"we've\", \"we have\", tweet)\n",
    "    tweet = re.sub(r\"it\\x89Ûªs\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"doesn\\x89Ûªt\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"It\\x89Ûªs\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Here\\x89Ûªs\", \"Here is\", tweet)\n",
    "    tweet = re.sub(r\"who's\", \"who is\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªve\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"y'all\", \"you all\", tweet)\n",
    "    tweet = re.sub(r\"can\\x89Ûªt\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"would've\", \"would have\", tweet)\n",
    "    tweet = re.sub(r\"it'll\", \"it will\", tweet)\n",
    "    tweet = re.sub(r\"we'll\", \"we will\", tweet)\n",
    "    tweet = re.sub(r\"wouldn\\x89Ûªt\", \"would not\", tweet)\n",
    "    tweet = re.sub(r\"We've\", \"We have\", tweet)\n",
    "    tweet = re.sub(r\"he'll\", \"he will\", tweet)\n",
    "    tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n",
    "    tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n",
    "    tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n",
    "    tweet = re.sub(r\"they'll\", \"they will\", tweet)\n",
    "    tweet = re.sub(r\"they'd\", \"they would\", tweet)\n",
    "    tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n",
    "    tweet = re.sub(r\"That\\x89Ûªs\", \"That is\", tweet)\n",
    "    tweet = re.sub(r\"they've\", \"they have\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"should've\", \"should have\", tweet)\n",
    "    tweet = re.sub(r\"You\\x89Ûªre\", \"You are\", tweet)\n",
    "    tweet = re.sub(r\"where's\", \"where is\", tweet)\n",
    "    tweet = re.sub(r\"Don\\x89Ûªt\", \"Do not\", tweet)\n",
    "    tweet = re.sub(r\"we'd\", \"we would\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"weren't\", \"were not\", tweet)\n",
    "    tweet = re.sub(r\"They're\", \"They are\", tweet)\n",
    "    tweet = re.sub(r\"Can\\x89Ûªt\", \"Cannot\", tweet)\n",
    "    tweet = re.sub(r\"you\\x89Ûªll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I\\x89Ûªd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"let's\", \"let us\", tweet)\n",
    "    tweet = re.sub(r\"it's\", \"it is\", tweet)\n",
    "    tweet = re.sub(r\"can't\", \"cannot\", tweet)\n",
    "    tweet = re.sub(r\"don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"you're\", \"you are\", tweet)\n",
    "    tweet = re.sub(r\"i've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"that's\", \"that is\", tweet)\n",
    "    tweet = re.sub(r\"i'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n",
    "    tweet = re.sub(r\"i'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"didn't\", \"did not\", tweet)\n",
    "    tweet = re.sub(r\"ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"you'll\", \"you will\", tweet)\n",
    "    tweet = re.sub(r\"I've\", \"I have\", tweet)\n",
    "    tweet = re.sub(r\"Don't\", \"do not\", tweet)\n",
    "    tweet = re.sub(r\"I'll\", \"I will\", tweet)\n",
    "    tweet = re.sub(r\"I'd\", \"I would\", tweet)\n",
    "    tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n",
    "    tweet = re.sub(r\"you'd\", \"You would\", tweet)\n",
    "    tweet = re.sub(r\"It's\", \"It is\", tweet)\n",
    "    tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n",
    "    tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n",
    "    tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n",
    "    tweet = re.sub(r\"youve\", \"you have\", tweet)  \n",
    "    tweet = re.sub(r\"donå«t\", \"do not\", tweet)   \n",
    "            \n",
    "    # Character entity references\n",
    "    tweet = re.sub(r\"&gt;\", \">\", tweet)\n",
    "    tweet = re.sub(r\"&lt;\", \"<\", tweet)\n",
    "    tweet = re.sub(r\"&amp;\", \"&\", tweet)\n",
    "    \n",
    "    # Typos, slang and informal abbreviations\n",
    "    tweet = re.sub(r\"w/e\", \"whatever\", tweet)\n",
    "    tweet = re.sub(r\"w/\", \"with\", tweet)\n",
    "    tweet = re.sub(r\"USAgov\", \"USA government\", tweet)\n",
    "    tweet = re.sub(r\"recentlu\", \"recently\", tweet)\n",
    "    tweet = re.sub(r\"Ph0tos\", \"Photos\", tweet)\n",
    "    tweet = re.sub(r\"amirite\", \"am I right\", tweet)\n",
    "    tweet = re.sub(r\"exp0sed\", \"exposed\", tweet)\n",
    "    tweet = re.sub(r\"<3\", \"love\", tweet)\n",
    "    tweet = re.sub(r\"amageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Trfc\", \"Traffic\", tweet)\n",
    "    tweet = re.sub(r\"8/5/2015\", \"2015-08-05\", tweet)\n",
    "    tweet = re.sub(r\"WindStorm\", \"Wind Storm\", tweet)\n",
    "    tweet = re.sub(r\"8/6/2015\", \"2015-08-06\", tweet)\n",
    "    tweet = re.sub(r\"10:38PM\", \"10:38 PM\", tweet)\n",
    "    tweet = re.sub(r\"10:30pm\", \"10:30 PM\", tweet)\n",
    "    tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n",
    "    tweet = re.sub(r\"lmao\", \"laughing my ass off\", tweet)   \n",
    "    tweet = re.sub(r\"TRAUMATISED\", \"traumatized\", tweet)\n",
    "    \n",
    "    # Hashtags and usernames\n",
    "    tweet = re.sub(r\"IranDeal\", \"Iran Deal\", tweet)\n",
    "    tweet = re.sub(r\"ArianaGrande\", \"Ariana Grande\", tweet)\n",
    "    tweet = re.sub(r\"camilacabello97\", \"camila cabello\", tweet) \n",
    "    tweet = re.sub(r\"RondaRousey\", \"Ronda Rousey\", tweet)     \n",
    "    tweet = re.sub(r\"MTVHottest\", \"MTV Hottest\", tweet)\n",
    "    tweet = re.sub(r\"TrapMusic\", \"Trap Music\", tweet)\n",
    "    tweet = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", tweet)\n",
    "    tweet = re.sub(r\"PantherAttack\", \"Panther Attack\", tweet)\n",
    "    tweet = re.sub(r\"StrategicPatience\", \"Strategic Patience\", tweet)\n",
    "    tweet = re.sub(r\"socialnews\", \"social news\", tweet)\n",
    "    tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n",
    "    tweet = re.sub(r\"onlinecommunities\", \"online communities\", tweet)\n",
    "    tweet = re.sub(r\"humanconsumption\", \"human consumption\", tweet)\n",
    "    tweet = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", tweet)\n",
    "    tweet = re.sub(r\"Meat-Loving\", \"Meat Loving\", tweet)\n",
    "    tweet = re.sub(r\"facialabuse\", \"facial abuse\", tweet)\n",
    "    tweet = re.sub(r\"LakeCounty\", \"Lake County\", tweet)\n",
    "    tweet = re.sub(r\"BeingAuthor\", \"Being Author\", tweet)\n",
    "    tweet = re.sub(r\"withheavenly\", \"with heavenly\", tweet)\n",
    "    tweet = re.sub(r\"thankU\", \"thank you\", tweet)\n",
    "    tweet = re.sub(r\"iTunesMusic\", \"iTunes Music\", tweet)\n",
    "    tweet = re.sub(r\"OffensiveContent\", \"Offensive Content\", tweet)\n",
    "    tweet = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", tweet)\n",
    "    tweet = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", tweet)\n",
    "    tweet = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", tweet)\n",
    "    tweet = re.sub(r\"animalrescue\", \"animal rescue\", tweet)\n",
    "    tweet = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", tweet)\n",
    "    tweet = re.sub(r\"aRmageddon\", \"armageddon\", tweet)\n",
    "    tweet = re.sub(r\"Throwingknifes\", \"Throwing knives\", tweet)\n",
    "    tweet = re.sub(r\"GodsLove\", \"God's Love\", tweet)\n",
    "    tweet = re.sub(r\"bookboost\", \"book boost\", tweet)\n",
    "    tweet = re.sub(r\"ibooklove\", \"I book love\", tweet)\n",
    "    tweet = re.sub(r\"NestleIndia\", \"Nestle India\", tweet)\n",
    "    tweet = re.sub(r\"realDonaldTrump\", \"Donald Trump\", tweet)\n",
    "    tweet = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", tweet)\n",
    "    tweet = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", tweet)\n",
    "    tweet = re.sub(r\"weathernetwork\", \"weather network\", tweet)\n",
    "    tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n",
    "    tweet = re.sub(r\"Hostage&2\", \"Hostage & 2\", tweet)\n",
    "    tweet = re.sub(r\"GOPDebate\", \"GOP Debate\", tweet)\n",
    "    tweet = re.sub(r\"RickPerry\", \"Rick Perry\", tweet)\n",
    "    tweet = re.sub(r\"frontpage\", \"front page\", tweet)\n",
    "    tweet = re.sub(r\"NewsInTweets\", \"News In Tweets\", tweet)\n",
    "    tweet = re.sub(r\"ViralSpell\", \"Viral Spell\", tweet)\n",
    "    tweet = re.sub(r\"til_now\", \"until now\", tweet)\n",
    "    tweet = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", tweet)\n",
    "    tweet = re.sub(r\"ZippedNews\", \"Zipped News\", tweet)\n",
    "    tweet = re.sub(r\"MicheleBachman\", \"Michele Bachman\", tweet)\n",
    "    tweet = re.sub(r\"53inch\", \"53 inch\", tweet)\n",
    "    tweet = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", tweet)\n",
    "    tweet = re.sub(r\"abstorm\", \"Alberta Storm\", tweet)\n",
    "    tweet = re.sub(r\"Beyhive\", \"Beyonce hive\", tweet)\n",
    "    tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n",
    "    tweet = re.sub(r\"DETECTADO\", \"Detected\", tweet)\n",
    "    tweet = re.sub(r\"RockyFire\", \"Rocky Fire\", tweet)\n",
    "    tweet = re.sub(r\"Listen/Buy\", \"Listen / Buy\", tweet)\n",
    "    tweet = re.sub(r\"NickCannon\", \"Nick Cannon\", tweet)\n",
    "    tweet = re.sub(r\"FaroeIslands\", \"Faroe Islands\", tweet)\n",
    "    tweet = re.sub(r\"yycstorm\", \"Calgary Storm\", tweet)\n",
    "    tweet = re.sub(r\"IDPs:\", \"Internally Displaced People :\", tweet)\n",
    "    tweet = re.sub(r\"ArtistsUnited\", \"Artists United\", tweet)\n",
    "    tweet = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", tweet)\n",
    "    tweet = re.sub(r\"jimmyfallon\", \"jimmy fallon\", tweet)\n",
    "    tweet = re.sub(r\"justinbieber\", \"justin bieber\", tweet)  \n",
    "    tweet = re.sub(r\"UTC2015\", \"UTC 2015\", tweet)\n",
    "    tweet = re.sub(r\"Time2015\", \"Time 2015\", tweet)\n",
    "    tweet = re.sub(r\"djicemoon\", \"dj icemoon\", tweet)\n",
    "    tweet = re.sub(r\"LivingSafely\", \"Living Safely\", tweet)\n",
    "    tweet = re.sub(r\"FIFA16\", \"Fifa 2016\", tweet)\n",
    "    tweet = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", tweet)\n",
    "    tweet = re.sub(r\"bbcnews\", \"bbc news\", tweet)\n",
    "    tweet = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", tweet)\n",
    "    tweet = re.sub(r\"c4news\", \"c4 news\", tweet)\n",
    "    tweet = re.sub(r\"OBLITERATION\", \"obliteration\", tweet)\n",
    "    tweet = re.sub(r\"MUDSLIDE\", \"mudslide\", tweet)\n",
    "    tweet = re.sub(r\"NoSurrender\", \"No Surrender\", tweet)\n",
    "    tweet = re.sub(r\"NotExplained\", \"Not Explained\", tweet)\n",
    "    tweet = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", tweet)\n",
    "    tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n",
    "    tweet = re.sub(r\"KOTAWeather\", \"KOTA Weather\", tweet)\n",
    "    tweet = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", tweet)\n",
    "    tweet = re.sub(r\"KOIN6News\", \"KOIN 6 News\", tweet)\n",
    "    tweet = re.sub(r\"LiveOnK2\", \"Live On K2\", tweet)\n",
    "    tweet = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", tweet)\n",
    "    tweet = re.sub(r\"nikeplus\", \"nike plus\", tweet)\n",
    "    tweet = re.sub(r\"david_cameron\", \"David Cameron\", tweet)\n",
    "    tweet = re.sub(r\"peterjukes\", \"Peter Jukes\", tweet)\n",
    "    tweet = re.sub(r\"JamesMelville\", \"James Melville\", tweet)\n",
    "    tweet = re.sub(r\"megynkelly\", \"Megyn Kelly\", tweet)\n",
    "    tweet = re.sub(r\"cnewslive\", \"C News Live\", tweet)\n",
    "    tweet = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", tweet)\n",
    "    tweet = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", tweet)\n",
    "    tweet = re.sub(r\"cbplawyers\", \"cbp lawyers\", tweet)\n",
    "    tweet = re.sub(r\"fewmoretweets\", \"few more tweets\", tweet)\n",
    "    tweet = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"cjoyner\", \"Chris Joyner\", tweet)\n",
    "    tweet = re.sub(r\"ENGvAUS\", \"England vs Australia\", tweet)\n",
    "    tweet = re.sub(r\"ScottWalker\", \"Scott Walker\", tweet)\n",
    "    tweet = re.sub(r\"MikeParrActor\", \"Michael Parr\", tweet)\n",
    "    tweet = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", tweet)\n",
    "    tweet = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", tweet)\n",
    "    tweet = re.sub(r\"realmandyrain\", \"Mandy Rain\", tweet)\n",
    "    tweet = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", tweet)\n",
    "    tweet = re.sub(r\"ApolloBrown\", \"Apollo Brown\", tweet)\n",
    "    tweet = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", tweet)\n",
    "    tweet = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", tweet)\n",
    "    tweet = re.sub(r\"AbbsWinston\", \"Abbs Winston\", tweet)\n",
    "    tweet = re.sub(r\"ShaunKing\", \"Shaun King\", tweet)\n",
    "    tweet = re.sub(r\"MeekMill\", \"Meek Mill\", tweet)\n",
    "    tweet = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", tweet)\n",
    "    tweet = re.sub(r\"GRupdates\", \"GR updates\", tweet)\n",
    "    tweet = re.sub(r\"SouthDowns\", \"South Downs\", tweet)\n",
    "    tweet = re.sub(r\"braininjury\", \"brain injury\", tweet)\n",
    "    tweet = re.sub(r\"auspol\", \"Australian politics\", tweet)\n",
    "    tweet = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", tweet)\n",
    "    tweet = re.sub(r\"calgaryweather\", \"Calgary Weather\", tweet)\n",
    "    tweet = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", tweet)\n",
    "    tweet = re.sub(r\"edsheeran\", \"Ed Sheeran\", tweet)\n",
    "    tweet = re.sub(r\"TrueHeroes\", \"True Heroes\", tweet)\n",
    "    tweet = re.sub(r\"S3XLEAK\", \"sex leak\", tweet)\n",
    "    tweet = re.sub(r\"ComplexMag\", \"Complex Magazine\", tweet)\n",
    "    tweet = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"CityofCalgary\", \"City of Calgary\", tweet)\n",
    "    tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n",
    "    tweet = re.sub(r\"SummerFate\", \"Summer Fate\", tweet)\n",
    "    tweet = re.sub(r\"RAmag\", \"Royal Academy Magazine\", tweet)\n",
    "    tweet = re.sub(r\"offers2go\", \"offers to go\", tweet)\n",
    "    tweet = re.sub(r\"foodscare\", \"food scare\", tweet)\n",
    "    tweet = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", tweet)\n",
    "    tweet = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", tweet)\n",
    "    tweet = re.sub(r\"GamerGate\", \"Gamer Gate\", tweet)\n",
    "    tweet = re.sub(r\"IHHen\", \"Humanitarian Relief\", tweet)\n",
    "    tweet = re.sub(r\"spinningbot\", \"spinning bot\", tweet)\n",
    "    tweet = re.sub(r\"ModiMinistry\", \"Modi Ministry\", tweet)\n",
    "    tweet = re.sub(r\"TAXIWAYS\", \"taxi ways\", tweet)\n",
    "    tweet = re.sub(r\"Calum5SOS\", \"Calum Hood\", tweet)\n",
    "    tweet = re.sub(r\"po_st\", \"po.st\", tweet)\n",
    "    tweet = re.sub(r\"scoopit\", \"scoop.it\", tweet)\n",
    "    tweet = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", tweet)\n",
    "    tweet = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", tweet)\n",
    "    tweet = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", tweet)\n",
    "    tweet = re.sub(r\"rapidcity\", \"Rapid City\", tweet)\n",
    "    tweet = re.sub(r\"OutBid\", \"outbid\", tweet)\n",
    "    tweet = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", tweet)\n",
    "    tweet = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", tweet)\n",
    "    tweet = re.sub(r\"15PM\", \"15 PM\", tweet)\n",
    "    tweet = re.sub(r\"OriginalFunko\", \"Funko\", tweet)\n",
    "    tweet = re.sub(r\"rightwaystan\", \"Richard Tan\", tweet)\n",
    "    tweet = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", tweet)\n",
    "    tweet = re.sub(r\"RT_America\", \"RT America\", tweet)\n",
    "    tweet = re.sub(r\"narendramodi\", \"Narendra Modi\", tweet)\n",
    "    tweet = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", tweet)\n",
    "    tweet = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", tweet)\n",
    "    tweet = re.sub(r\"alexbelloli\", \"Alex Belloli\", tweet)\n",
    "    tweet = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", tweet)\n",
    "    tweet = re.sub(r\"gunsense\", \"gun sense\", tweet)\n",
    "    tweet = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", tweet)\n",
    "    tweet = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", tweet)\n",
    "    tweet = re.sub(r\"samanthaturne19\", \"Samantha Turner\", tweet)\n",
    "    tweet = re.sub(r\"JonVoyage\", \"Jon Stewart\", tweet)\n",
    "    tweet = re.sub(r\"renew911health\", \"renew 911 health\", tweet)\n",
    "    tweet = re.sub(r\"SuryaRay\", \"Surya Ray\", tweet)\n",
    "    tweet = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", tweet)\n",
    "    tweet = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", tweet)\n",
    "    tweet = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", tweet)\n",
    "    tweet = re.sub(r\"pmarca\", \"Marc Andreessen\", tweet)\n",
    "    tweet = re.sub(r\"pdx911\", \"Portland Police\", tweet)\n",
    "    tweet = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", tweet)\n",
    "    tweet = re.sub(r\"Japton\", \"Arkansas\", tweet)\n",
    "    tweet = re.sub(r\"RouteComplex\", \"Route Complex\", tweet)\n",
    "    tweet = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", tweet)\n",
    "    tweet = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", tweet)\n",
    "    tweet = re.sub(r\"Politifiact\", \"PolitiFact\", tweet)\n",
    "    tweet = re.sub(r\"Hiroshima70\", \"Hiroshima\", tweet)\n",
    "    tweet = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", tweet)\n",
    "    tweet = re.sub(r\"versethe\", \"verse the\", tweet)\n",
    "    tweet = re.sub(r\"TubeStrike\", \"Tube Strike\", tweet)\n",
    "    tweet = re.sub(r\"MissionHills\", \"Mission Hills\", tweet)\n",
    "    tweet = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", tweet)\n",
    "    tweet = re.sub(r\"NANKANA\", \"Nankana\", tweet)\n",
    "    tweet = re.sub(r\"SAHIB\", \"Sahib\", tweet)\n",
    "    tweet = re.sub(r\"PAKPATTAN\", \"Pakpattan\", tweet)\n",
    "    tweet = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", tweet)\n",
    "    tweet = re.sub(r\"gofundme\", \"go fund me\", tweet)\n",
    "    tweet = re.sub(r\"pmharper\", \"Stephen Harper\", tweet)\n",
    "    tweet = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", tweet)\n",
    "    tweet = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", tweet)\n",
    "    tweet = re.sub(r\"bancodeseries\", \"banco de series\", tweet)\n",
    "    tweet = re.sub(r\"timkaine\", \"Tim Kaine\", tweet)\n",
    "    tweet = re.sub(r\"IdentityTheft\", \"Identity Theft\", tweet)\n",
    "    tweet = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", tweet)\n",
    "    tweet = re.sub(r\"mishacollins\", \"Misha Collins\", tweet)\n",
    "    tweet = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", tweet)\n",
    "    tweet = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", tweet)\n",
    "    tweet = re.sub(r\"Kowing\", \"Knowing\", tweet)\n",
    "    tweet = re.sub(r\"ScreamQueens\", \"Scream Queens\", tweet)\n",
    "    tweet = re.sub(r\"AskCharley\", \"Ask Charley\", tweet)\n",
    "    tweet = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", tweet)\n",
    "    tweet = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", tweet)\n",
    "    tweet = re.sub(r\"HannaPH\", \"Typhoon Hanna\", tweet)\n",
    "    tweet = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", tweet)\n",
    "    tweet = re.sub(r\"Ptbo\", \"Peterborough\", tweet)\n",
    "    tweet = re.sub(r\"cnnbrk\", \"CNN Breaking News\", tweet)\n",
    "    tweet = re.sub(r\"IndianNews\", \"Indian News\", tweet)\n",
    "    tweet = re.sub(r\"savebees\", \"save bees\", tweet)\n",
    "    tweet = re.sub(r\"GreenHarvard\", \"Green Harvard\", tweet)\n",
    "    tweet = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", tweet)\n",
    "    tweet = re.sub(r\"hermancranston\", \"Herman Cranston\", tweet)\n",
    "    tweet = re.sub(r\"WMUR9\", \"WMUR-TV\", tweet)\n",
    "    tweet = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", tweet)\n",
    "    tweet = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", tweet)\n",
    "    tweet = re.sub(r\"ProSyn\", \"Project Syndicate\", tweet)\n",
    "    tweet = re.sub(r\"Daesh\", \"ISIS\", tweet)\n",
    "    tweet = re.sub(r\"s2g\", \"swear to god\", tweet)\n",
    "    tweet = re.sub(r\"listenlive\", \"listen live\", tweet)\n",
    "    tweet = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", tweet)\n",
    "    tweet = re.sub(r\"FoxNew\", \"Fox News\", tweet)\n",
    "    tweet = re.sub(r\"CBSBigBrother\", \"Big Brother\", tweet)\n",
    "    tweet = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", tweet)\n",
    "    tweet = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", tweet)\n",
    "    tweet = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", tweet)\n",
    "    tweet = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", tweet)\n",
    "    tweet = re.sub(r\"Popularmmos\", \"Popular MMOs\", tweet)\n",
    "    tweet = re.sub(r\"WildHorses\", \"Wild Horses\", tweet)\n",
    "    tweet = re.sub(r\"FantasticFour\", \"Fantastic Four\", tweet)\n",
    "    tweet = re.sub(r\"HORNDALE\", \"Horndale\", tweet)\n",
    "    tweet = re.sub(r\"PINER\", \"Piner\", tweet)\n",
    "    tweet = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", tweet)\n",
    "    tweet = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", tweet)\n",
    "    tweet = re.sub(r\"residualincome\", \"residual income\", tweet)\n",
    "    tweet = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", tweet)\n",
    "    tweet = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", tweet)\n",
    "    tweet = re.sub(r\"AmazonDeals\", \"Amazon Deals\", tweet)\n",
    "    tweet = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", tweet)\n",
    "    tweet = re.sub(r\"shoalstraffic\", \"shoals traffic\", tweet)\n",
    "    tweet = re.sub(r\"GeorgeFoster72\", \"George Foster\", tweet)\n",
    "    tweet = re.sub(r\"pop2015\", \"pop 2015\", tweet)\n",
    "    tweet = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", tweet)\n",
    "    tweet = re.sub(r\"DianneG\", \"Dianne Gallagher\", tweet)\n",
    "    tweet = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", tweet)\n",
    "    tweet = re.sub(r\"BritishBakeOff\", \"British Bake Off\", tweet)\n",
    "    tweet = re.sub(r\"FreeKashmir\", \"Free Kashmir\", tweet)\n",
    "    tweet = re.sub(r\"mattmosley\", \"Matt Mosley\", tweet)\n",
    "    tweet = re.sub(r\"BishopFred\", \"Bishop Fred\", tweet)\n",
    "    tweet = re.sub(r\"EndConflict\", \"End Conflict\", tweet)\n",
    "    tweet = re.sub(r\"EndOccupation\", \"End Occupation\", tweet)\n",
    "    tweet = re.sub(r\"UNHEALED\", \"unhealed\", tweet)\n",
    "    tweet = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", tweet)\n",
    "    tweet = re.sub(r\"Latestnews\", \"Latest news\", tweet)\n",
    "    tweet = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", tweet)\n",
    "    tweet = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", tweet)\n",
    "    tweet = re.sub(r\"datingtips\", \"dating tips\", tweet)\n",
    "    tweet = re.sub(r\"charlesadler\", \"Charles Adler\", tweet)\n",
    "    tweet = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", tweet)\n",
    "    tweet = re.sub(r\"txlege\", \"Texas Legislature\", tweet)\n",
    "    tweet = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", tweet)\n",
    "           \n",
    "    # Urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ')      \n",
    "        \n",
    "    # Acronyms\n",
    "    tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", tweet)\n",
    "    tweet = re.sub(r\"mÌ¼sica\", \"music\", tweet)\n",
    "    tweet = re.sub(r\"okwx\", \"Oklahoma City Weather\", tweet)\n",
    "    tweet = re.sub(r\"arwx\", \"Arkansas Weather\", tweet)    \n",
    "    tweet = re.sub(r\"gawx\", \"Georgia Weather\", tweet)  \n",
    "    tweet = re.sub(r\"scwx\", \"South Carolina Weather\", tweet)  \n",
    "    tweet = re.sub(r\"cawx\", \"California Weather\", tweet)\n",
    "    tweet = re.sub(r\"tnwx\", \"Tennessee Weather\", tweet)\n",
    "    tweet = re.sub(r\"azwx\", \"Arizona Weather\", tweet)    \n",
    "    tweet = re.sub(r\"wordpressdotcom\", \"wordpress\", tweet)    \n",
    "    tweet = re.sub(r\"usNWSgov\", \"United States National Weather Service\", tweet)\n",
    "    tweet = re.sub(r\"Suruc\", \"Sanliurfa\", tweet)   \n",
    "    \n",
    "    # Grouping same words without embeddings\n",
    "    tweet = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", tweet)\n",
    "    tweet = re.sub(r\"SOUDELOR\", \"Soudelor\", tweet)\n",
    "    \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \n",
       "0       1      deeds reason #earthquake may allah forgive us  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  residents asked shelter place notified officer...  \n",
       "3       1  13,000 people receive #wildfires evacuation or...  \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['clean_text1']=data_train['clean_text'].apply(lambda x:clean1(x))\n",
    "data_test['clean_text1']=data_test['clean'].apply(lambda x:clean1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deeds reason  # earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>got sent photo ruby  # alaska smoke  # wildfir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1      deeds reason #earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked shelter place notified officer...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...   \n",
       "\n",
       "                                         clean_text1  \n",
       "0    deeds reason  # earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  residents asked shelter place notified officer...  \n",
       "3  13,000 people receive  # wildfires evacuation ...  \n",
       "4  got sent photo ruby  # alaska smoke  # wildfir...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_train.clean_text1.values\n",
    "Y=data_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vect=TfidfVectorizer(max_features=3000)\n",
    "x_train_bow=count_vect.fit_transform(X_train)\n",
    "x_test_bow=count_vect.transform(X_test)\n",
    "x_ttest_bow=count_vect.transform(data_test.clean.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851, 3000) (762, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_bow.shape,x_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 : 77.69028871391076\n",
      "0.2 : 77.69028871391076\n",
      "0.01 : 76.9028871391076\n",
      "0.5 : 77.03412073490814\n",
      "0.05 : 77.16535433070865\n",
      "1 : 76.77165354330708\n",
      "1.5 : 76.9028871391076\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alpa=[0.1,0.2,0.01,0.5,0.05,1,1.5]\n",
    "for i in alpa:\n",
    "    nb=MultinomialNB(alpha=i)\n",
    "    nb.fit(x_train_bow,y_train)\n",
    "    print(i,\":\",accuracy_score(y_test,nb.predict(x_test_bow))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.64041994750657"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "sv=LinearSVC()\n",
    "sv.fit(x_train_bow,y_train)\n",
    "accuracy_score(y_test,sv.predict(x_test_bow))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851, 1200) (762, 1200)\n",
      "0.1 : 76.50918635170603\n",
      "0.2 : 76.50918635170603\n",
      "0.01 : 76.64041994750657\n",
      "0.5 : 76.77165354330708\n",
      "0.05 : 76.64041994750657\n",
      "1 : 76.77165354330708\n",
      "1.5 : 77.16535433070865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "76.24671916010499"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data_train.clean_text1.values\n",
    "Y=data_train['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.10,random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vect=TfidfVectorizer(max_features=1200)\n",
    "x_train_bow=count_vect.fit_transform(X_train)\n",
    "x_test_bow=count_vect.transform(X_test)\n",
    "x_ttest_bow=count_vect.transform(data_test.clean.values)\n",
    "\n",
    "print(x_train_bow.shape,x_test_bow.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alpa=[0.1,0.2,0.01,0.5,0.05,1,1.5]\n",
    "for i in alpa:\n",
    "    nb=MultinomialNB(alpha=i)\n",
    "    nb.fit(x_train_bow,y_train)\n",
    "    print(i,\":\",accuracy_score(y_test,nb.predict(x_test_bow))*100)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "sv=LinearSVC()\n",
    "sv.fit(x_train_bow,y_train)\n",
    "accuracy_score(y_test,sv.predict(x_test_bow))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deeds reason  # earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>got sent photo ruby  # alaska smoke  # wildfir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1      deeds reason #earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked shelter place notified officer...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...   \n",
       "\n",
       "                                         clean_text1  \n",
       "0    deeds reason  # earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  residents asked shelter place notified officer...  \n",
       "3  13,000 people receive  # wildfires evacuation ...  \n",
       "4  got sent photo ruby  # alaska smoke  # wildfir...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>clean_text1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "      <td>happened terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>heard #earthquake different cities stay safe e...</td>\n",
       "      <td>heard  # earthquake different cities stay safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "      <td>forest fire spot pond geese fleeing across str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>apocalypse lighting #spokane #wildfires</td>\n",
       "      <td>apocalypse lighting  # spokane  # wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "      <td>typhoon soudelor kills 28 china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "                                               clean  \\\n",
       "0                        happened terrible car crash   \n",
       "1  heard #earthquake different cities stay safe e...   \n",
       "2  forest fire spot pond geese fleeing across str...   \n",
       "3            apocalypse lighting #spokane #wildfires   \n",
       "4             typhoon soudelor kills 28 china taiwan   \n",
       "\n",
       "                                         clean_text1  \n",
       "0                        happened terrible car crash  \n",
       "1  heard  # earthquake different cities stay safe...  \n",
       "2  forest fire spot pond geese fleeing across str...  \n",
       "3        apocalypse lighting  # spokane  # wildfires  \n",
       "4             typhoon soudelor kills 28 china taiwan  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['word_count']=data_train['clean_text1'].apply(lambda x:len(wt(x)))\n",
    "data_test['word_count']=data_test['clean_text1'].apply(lambda x:len(wt(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text1</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deeds reason  # earthquake may allah forgive us</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>got sent photo ruby  # alaska smoke  # wildfir...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1      deeds reason #earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked shelter place notified officer...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...   \n",
       "\n",
       "                                         clean_text1  word_count  \n",
       "0    deeds reason  # earthquake may allah forgive us           8  \n",
       "1              forest fire near la ronge sask canada           7  \n",
       "2  residents asked shelter place notified officer...          11  \n",
       "3  13,000 people receive  # wildfires evacuation ...           8  \n",
       "4  got sent photo ruby  # alaska smoke  # wildfir...          11  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['pun_count']=data_train['clean_text'].apply(lambda x:len([i for i in wt(x) if i in pun]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>clean_text1</th>\n",
       "      <th>word_count</th>\n",
       "      <th>pun_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason #earthquake may allah forgive us</td>\n",
       "      <td>deeds reason  # earthquake may allah forgive us</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>13,000 people receive  # wildfires evacuation ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo ruby #alaska smoke #wildfires p...</td>\n",
       "      <td>got sent photo ruby  # alaska smoke  # wildfir...</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         clean_text  \\\n",
       "0       1      deeds reason #earthquake may allah forgive us   \n",
       "1       1              forest fire near la ronge sask canada   \n",
       "2       1  residents asked shelter place notified officer...   \n",
       "3       1  13,000 people receive #wildfires evacuation or...   \n",
       "4       1  got sent photo ruby #alaska smoke #wildfires p...   \n",
       "\n",
       "                                         clean_text1  word_count  pun_count  \n",
       "0    deeds reason  # earthquake may allah forgive us           8          1  \n",
       "1              forest fire near la ronge sask canada           7          0  \n",
       "2  residents asked shelter place notified officer...          11          0  \n",
       "3  13,000 people receive  # wildfires evacuation ...           8          1  \n",
       "4  got sent photo ruby  # alaska smoke  # wildfir...          11          2  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6851, 15837) (762, 15837)\n",
      "0.1 : 77.29658792650919\n",
      "0.2 : 77.16535433070865\n",
      "0.01 : 76.24671916010499\n",
      "0.5 : 77.29658792650919\n",
      "0.05 : 77.55905511811024\n",
      "1 : 77.69028871391076\n",
      "1.5 : 77.82152230971128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "77.03412073490814"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=data_train.clean_text1.values\n",
    "Y=data_train['target']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,Y,test_size=0.10,random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "count_vect=TfidfVectorizer()\n",
    "x_train_bow=count_vect.fit_transform(X_train)\n",
    "x_test_bow=count_vect.transform(X_test)\n",
    "x_ttest_bow=count_vect.transform(data_test.clean.values)\n",
    "\n",
    "print(x_train_bow.shape,x_test_bow.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "alpa=[0.1,0.2,0.01,0.5,0.05,1,1.5]\n",
    "for i in alpa:\n",
    "    nb=MultinomialNB(alpha=i)\n",
    "    nb.fit(x_train_bow,y_train)\n",
    "    print(i,\":\",accuracy_score(y_test,nb.predict(x_test_bow))*100)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "sv=LinearSVC()\n",
    "sv.fit(x_train_bow,y_train)\n",
    "accuracy_score(y_test,sv.predict(x_test_bow))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
